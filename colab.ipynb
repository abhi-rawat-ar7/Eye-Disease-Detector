{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff4714e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Part 1: Model Training on Google Colab\n",
    "# This script will guide you through setting up Colab, downloading a Kaggle\n",
    "# dataset, preprocessing images, training a deep learning model, and saving it.\n",
    "# ==============================================================================\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Step 1: Setup and Dataset Download\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Mount Google Drive to store Kaggle API key and trained model\n",
    "# This allows Colab to access files in your Google Drive.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Important: Do NOT force specific TensorFlow, NumPy, or Protobuf versions here.\n",
    "# Let Google Colab use its default, stable, pre-installed versions.\n",
    "# Forcing older versions in a Python 3.11 environment leads to instability.\n",
    "# We will check the TensorFlow version after import and use that for deployment.\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Install Kaggle API client\n",
    "# Kaggle API is used to download datasets directly from Kaggle.\n",
    "!pip install kaggle\n",
    "# Ensure Pillow is installed, as it's common for image processing\n",
    "!pip install Pillow\n",
    "\n",
    "# Configure Kaggle API credentials\n",
    "# You need to upload your `kaggle.json` file to your Google Drive.\n",
    "# Go to Kaggle (kaggle.com), click on your profile picture, then \"Account\",\n",
    "# scroll down to \"API\", and click \"Create New API Token\" to download `kaggle.json`.\n",
    "# Then upload this file to the root of your Google Drive.\n",
    "import os\n",
    "import json\n",
    "import subprocess # Used for checking command errors\n",
    "\n",
    "# Define the path where the kaggle.json file should be placed\n",
    "kaggle_dir = '/root/.kaggle'\n",
    "if not os.path.exists(kaggle_dir):\n",
    "    os.makedirs(kaggle_dir)\n",
    "\n",
    "# Path to the kaggle.json file in Google Drive\n",
    "kaggle_json_drive_path = '/content/drive/MyDrive/kaggle.json'\n",
    "if not os.path.exists(kaggle_json_drive_path):\n",
    "    print(\"ERROR: kaggle.json not found in your Google Drive. Please upload it to /content/drive/MyDrive/kaggle.json\")\n",
    "    print(\"Go to Kaggle -> Account -> Create New API Token to download kaggle.json.\")\n",
    "    print(\"Ensure you accept the dataset's terms on Kaggle before attempting download.\")\n",
    "    exit() # Exit if API key is not found\n",
    "\n",
    "# Copy kaggle.json from Google Drive to the required path for Kaggle API\n",
    "try:\n",
    "    !cp {kaggle_json_drive_path} {kaggle_dir}/kaggle.json\n",
    "    # Set permissions for the kaggle.json file\n",
    "    !chmod 600 {kaggle_dir}/kaggle.json\n",
    "    print(\"Kaggle API key setup complete.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error copying or setting permissions for kaggle.json: {e}\")\n",
    "    print(\"Please ensure your Google Drive is mounted and kaggle.json is accessible.\")\n",
    "    exit() # Exit if API key setup fails\n",
    "\n",
    "\n",
    "print(\"Proceeding to dataset download.\")\n",
    "\n",
    "# Download the dataset\n",
    "# We'll use the \"1000 Fundus images with 39 categories\" dataset as specified.\n",
    "# Dataset URL: https://www.kaggle.com/datasets/siddharthm18/1000-fundus-images-with-39-categories\n",
    "kaggle_dataset_path = 'linchundan/fundusimage1000'\n",
    "dataset_download_path = '/content/fundus_dataset'\n",
    "\n",
    "# Create a directory for the dataset if it doesn't exist\n",
    "if not os.path.exists(dataset_download_path):\n",
    "    os.makedirs(dataset_download_path)\n",
    "\n",
    "# Change current directory to the dataset path for Kaggle download command\n",
    "%cd {dataset_download_path}\n",
    "# Download the dataset using Kaggle API\n",
    "try:\n",
    "    print(f\"Attempting to download dataset: {kaggle_dataset_path}\")\n",
    "    # Use -q (quiet) for quiet mode, which suppresses progress output\n",
    "    # check=True raises CalledProcessError on non-zero exit codes\n",
    "    result = subprocess.run(['kaggle', 'datasets', 'download', kaggle_dataset_path, '-q'], capture_output=True, text=True, check=True)\n",
    "    print(f\"Dataset downloaded successfully to: {dataset_download_path}\")\n",
    "\n",
    "    # Unzip the downloaded dataset\n",
    "    zip_file_name = os.path.basename(kaggle_dataset_path) + '.zip'\n",
    "    if os.path.exists(zip_file_name):\n",
    "        print(f\"Unzipping {zip_file_name}...\")\n",
    "        subprocess.run(['unzip', '-q', zip_file_name], check=True)\n",
    "        print(\"Dataset unzipped.\")\n",
    "    else:\n",
    "        print(f\"Error: Downloaded zip file '{zip_file_name}' not found. Dataset download might have failed silently.\")\n",
    "        print(\"Please check your Kaggle API key and ensure you have accepted the dataset terms on Kaggle.\")\n",
    "        exit() # Exit if zip file is not found\n",
    "\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"\\nERROR: Kaggle dataset download failed with exit code {e.returncode}.\")\n",
    "    print(f\"Command: {' '.join(e.cmd)}\")\n",
    "    # Check if stderr output exists before trying to decode/print it\n",
    "    if e.stderr:\n",
    "        print(f\"Error Output (stderr):\\n{e.stderr}\")\n",
    "    elif e.stdout: # Sometimes error might be in stdout\n",
    "        print(f\"Output (stdout):\\n{e.stdout}\")\n",
    "    else:\n",
    "        print(\"No specific error output from Kaggle command. This often means permission issues.\")\n",
    "\n",
    "    print(\"\\nCommon reasons for this error:\")\n",
    "    print(\"1. Your Kaggle API key (kaggle.json) might be incorrect or have wrong permissions.\")\n",
    "    print(\"2. You might not have accepted the terms & conditions for this dataset on Kaggle.\")\n",
    "    print(f\"   Please visit: https://www.kaggle.com/datasets/{kaggle_dataset_path} and accept any terms.\")\n",
    "    print(\"3. There might be a network issue or Kaggle API rate limit exceeded.\")\n",
    "    exit() # Exit the script if download fails\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during dataset download or unzip: {e}\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# List the contents of the dataset directory to verify\n",
    "print(f\"Contents of {dataset_download_path}:\")\n",
    "!ls -F {dataset_download_path}\n",
    "\n",
    "# Set the base directory for the dataset after unzipping\n",
    "# The images are typically in a folder named 'Dataset' inside the unzipped content.\n",
    "base_data_dir = os.path.join(dataset_download_path, 'Dataset')\n",
    "if not os.path.exists(base_data_dir):\n",
    "    print(f\"Warning: 'Dataset' directory not found at {base_data_dir}. Checking for other main directory.\")\n",
    "    # Try to find the actual directory containing images, usually a single folder\n",
    "    # after unzipping if not directly 'Dataset'.\n",
    "    subdirs = [d for d in os.listdir(dataset_download_path) if os.path.isdir(os.path.join(dataset_download_path, d))]\n",
    "    if subdirs:\n",
    "        # Prioritize 'Dataset' if it exists in subdirs, otherwise pick the first one\n",
    "        if 'Dataset' in subdirs:\n",
    "            base_data_dir = os.path.join(dataset_download_path, 'Dataset')\n",
    "        else:\n",
    "            base_data_dir = os.path.join(dataset_download_path, subdirs[0])\n",
    "        print(f\"Assuming image directory is: {base_data_dir}\")\n",
    "    else:\n",
    "        print(\"Error: Could not locate image directory within the unzipped dataset.\")\n",
    "        # Fallback to the main unzipped directory if no subdirectories are found\n",
    "        base_data_dir = dataset_download_path\n",
    "        print(f\"Falling back to: {base_data_dir}. This might not be the correct image folder.\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Step 2: Data Preprocessing\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf # Import TensorFlow\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tqdm import tqdm # For progress bars\n",
    "\n",
    "# Print TensorFlow version for later use in requirements.txt\n",
    "print(f\"TensorFlow version being used: {tf.__version__}\")\n",
    "\n",
    "print(\"\\nStarting data preprocessing...\")\n",
    "\n",
    "IMG_SIZE = 224 # Standard input size for many pre-trained CNNs\n",
    "BATCH_SIZE = 32 # Batch size for training\n",
    "NUM_CLASSES = 39 # As per the dataset description\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "image_paths = [] # Store paths for debugging or verification\n",
    "\n",
    "# Iterate through each subfolder (which represents a class)\n",
    "# Check if base_data_dir exists and is a directory\n",
    "if not os.path.isdir(base_data_dir):\n",
    "    print(f\"ERROR: Base data directory not found or is not a directory: {base_data_dir}\")\n",
    "    print(\"Please ensure the Kaggle dataset was downloaded and unzipped correctly, and the 'Dataset' folder (or equivalent) exists.\")\n",
    "    exit() # Exit if data directory is invalid\n",
    "\n",
    "for category in os.listdir(base_data_dir):\n",
    "    category_path = os.path.join(base_data_dir, category)\n",
    "    if os.path.isdir(category_path): # Ensure it's a directory\n",
    "        print(f\"Processing category: {category}\")\n",
    "        for img_name in os.listdir(category_path):\n",
    "            img_path = os.path.join(category_path, img_name)\n",
    "            try:\n",
    "                # Read image in BGR format (OpenCV default)\n",
    "                img = cv2.imread(img_path)\n",
    "                if img is None:\n",
    "                    print(f\"Warning: Could not read image {img_path}. Skipping.\")\n",
    "                    continue\n",
    "                # Resize image to IMG_SIZE x IMG_SIZE\n",
    "                img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "                # Convert BGR to RGB (TensorFlow/Keras usually expects RGB)\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                # Normalize pixel values to [0, 1]\n",
    "                img = img / 255.0\n",
    "\n",
    "                images.append(img)\n",
    "                labels.append(category)\n",
    "                image_paths.append(img_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {img_path}: {e}. Skipping.\")\n",
    "\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(f\"Total images loaded: {len(images)}\")\n",
    "print(f\"Total labels loaded: {len(labels)}\")\n",
    "\n",
    "# IMPORTANT: Check if any images were loaded before proceeding\n",
    "if len(images) == 0:\n",
    "    print(\"ERROR: No images were loaded. This usually means the dataset download failed or the path to images is incorrect.\")\n",
    "    print(\"Please review the error messages above related to dataset download and verify your Kaggle API key and dataset access.\")\n",
    "    exit() # Stop execution if no data is available\n",
    "\n",
    "# Encode labels to numerical format\n",
    "# LabelEncoder assigns a unique integer to each unique category string.\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded_labels = label_encoder.fit_transform(labels)\n",
    "print(f\"Classes found: {label_encoder.classes_}\")\n",
    "\n",
    "# Convert integer labels to one-hot encoded format\n",
    "# One-hot encoding is required for categorical cross-entropy loss in multi-class classification.\n",
    "one_hot_labels = to_categorical(integer_encoded_labels, num_classes=NUM_CLASSES)\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "# Stratify ensures that the proportion of classes is the same in all splits.\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    images, one_hot_labels, test_size=0.3, random_state=42, stratify=one_hot_labels\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Train set shape: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}, {y_val.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}, {y_test.shape}\")\n",
    "\n",
    "# Save the label encoder classes to be used later in the backend\n",
    "# This mapping is crucial to convert model predictions (integers) back to meaningful disease names.\n",
    "label_mapping = {int(i): label for i, label in enumerate(label_encoder.classes_)}\n",
    "label_mapping_path = '/content/drive/MyDrive/label_mapping.json'\n",
    "with open(label_mapping_path, 'w') as f:\n",
    "    json.dump(label_mapping, f)\n",
    "print(f\"Label mapping saved to {label_mapping_path}\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Step 3: Model Definition and Training\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# TensorFlow is already imported in Step 2, so no need to import it again here.\n",
    "from tensorflow.keras.applications import MobileNetV2 # Efficient for mobile/edge devices\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "print(\"\\nStarting model definition and training...\")\n",
    "\n",
    "# Load the pre-trained MobileNetV2 model\n",
    "# include_top=False means we don't include the classification layers of MobileNetV2,\n",
    "# allowing us to add our own for our specific number of classes.\n",
    "# weights='imagenet' uses weights pre-trained on the ImageNet dataset.\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "\n",
    "# Freeze the layers of the base model\n",
    "# This prevents the pre-trained weights from being updated during training\n",
    "# of the new classification layers, which helps in faster convergence and avoids\n",
    "# overfitting with smaller datasets.\n",
    "base_model.trainable = False\n",
    "\n",
    "# Add custom classification layers on top of the base model\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x) # Reduces spatial dimensions, suitable for classification\n",
    "x = Dense(256, activation='relu')(x) # A fully connected layer\n",
    "x = Dropout(0.5)(x) # Dropout for regularization to prevent overfitting\n",
    "predictions = Dense(NUM_CLASSES, activation='softmax')(x) # Output layer with softmax for multi-class classification\n",
    "\n",
    "# Create the full model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "# Adam optimizer is a popular choice.\n",
    "# categorical_crossentropy is used for one-hot encoded labels.\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Data Augmentation for training data\n",
    "# This creates new variations of training images on-the-fly,\n",
    "# further improving generalization and reducing overfitting.\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# No augmentation for validation and test data, only normalization handled earlier\n",
    "val_datagen = ImageDataGenerator() # Only used to flow data, no augmentation here\n",
    "\n",
    "# Prepare generators for training and validation data\n",
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=BATCH_SIZE)\n",
    "validation_generator = val_datagen.flow(X_val, y_val, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Callbacks for better training control\n",
    "# EarlyStopping stops training if validation accuracy doesn't improve for 'patience' epochs\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
    "# ModelCheckpoint saves the best model based on validation accuracy\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath='/content/drive/MyDrive/best_fundus_model.h5', # Path to save the best model\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "# Using generators for augmented data\n",
    "EPOCHS = 50 # Number of training epochs, can be adjusted\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(X_train) // BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=len(X_val) // BATCH_SIZE,\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")\n",
    "\n",
    "print(\"\\nModel training complete.\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Step 4: Model Evaluation and Saving\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Load the best saved model for evaluation\n",
    "from tensorflow.keras.models import load_model\n",
    "best_model_path = '/content/drive/MyDrive/best_fundus_model.h5'\n",
    "model = load_model(best_model_path) # Load the model with best validation accuracy\n",
    "\n",
    "print(f\"\\nEvaluating the best model loaded from: {best_model_path}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nModel training and saving complete! The best model is saved to Google Drive.\")\n",
    "print(\"The label mapping file is also saved to Google Drive, which is essential for the backend.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73acdd95",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf # Import TensorFlow\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tqdm import tqdm # For progress bars\n",
    "\n",
    "print(f\"TensorFlow version being used: {tf.__version__}\")\n",
    "\n",
    "print(\"\\nStarting data preprocessing...\")\n",
    "\n",
    "IMG_SIZE = 224 # Standard input size for many pre-trained CNNs\n",
    "BATCH_SIZE = 32 # Batch size for training\n",
    "NUM_CLASSES = 39 # As per the dataset description\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "image_paths = [] # Store paths for debugging or verification\n",
    "\n",
    "if not os.path.isdir(base_data_dir):\n",
    "    print(f\"ERROR: Base data directory not found or is not a directory: {base_data_dir}\")\n",
    "    print(\"Please ensure the Kaggle dataset was downloaded and unzipped correctly, and the 'Dataset' folder (or equivalent) exists.\")\n",
    "    exit()\n",
    "\n",
    "# Monitor this loop carefully for any printed errors\n",
    "for category in os.listdir(base_data_dir):\n",
    "    category_path = os.path.join(base_data_dir, category)\n",
    "    if os.path.isdir(category_path):\n",
    "        print(f\"Processing category: {category}\") # This should print for each category\n",
    "        for img_name in os.listdir(category_path):\n",
    "            img_path = os.path.join(category_path, img_name)\n",
    "            try:\n",
    "                img = cv2.imread(img_path)\n",
    "                if img is None:\n",
    "                    print(f\"Warning: Could not read image {img_path}. Skipping.\")\n",
    "                    continue\n",
    "                img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                img = img / 255.0\n",
    "\n",
    "                images.append(img)\n",
    "                labels.append(category)\n",
    "                image_paths.append(img_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {img_path}: {e}. Skipping.\")\n",
    "\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(f\"Total images loaded: {len(images)}\") # IMPORTANT: This should be > 0\n",
    "print(f\"Total labels loaded: {len(labels)}\") # IMPORTANT: This should be > 0\n",
    "\n",
    "if len(images) == 0:\n",
    "    print(\"ERROR: No images were loaded. This usually means the dataset download failed or the path to images is incorrect.\")\n",
    "    print(\"Please review the error messages above related to dataset download and verify your Kaggle API key and dataset access.\")\n",
    "    exit()\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded_labels = label_encoder.fit_transform(labels)\n",
    "print(f\"Classes found: {label_encoder.classes_}\")\n",
    "\n",
    "one_hot_labels = to_categorical(integer_encoded_labels, num_classes=NUM_CLASSES)\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    images, one_hot_labels, test_size=0.3, random_state=42, stratify=one_hot_labels\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Train set shape: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}, {y_val.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}, {y_test.shape}\")\n",
    "\n",
    "# This is the crucial part for label_mapping.json\n",
    "label_mapping = {int(i): label for i, label in enumerate(label_encoder.classes_)}\n",
    "label_mapping_path = '/content/drive/MyDrive/label_mapping.json'\n",
    "try:\n",
    "    with open(label_mapping_path, 'w') as f:\n",
    "        json.dump(label_mapping, f)\n",
    "    print(f\"Label mapping saved to {label_mapping_path}\") # Look for this line!\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not save label_mapping.json: {e}\") # Any error here?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
